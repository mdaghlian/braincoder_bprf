{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3246db",
   "metadata": {},
   "source": [
    "# Check have I implemented the nystrom approximation speed up correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4e49f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 16:19:36.504628: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-03 16:19:36.509357: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-03 16:19:36.519479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756909176.536406 3586931 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756909176.541528 3586931 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756909176.556251 3586931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756909176.556264 3586931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756909176.556266 3586931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756909176.556268 3586931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-03 16:19:36.561315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "# ******************* return log probability ****************\n",
    "# - distance based gp prior \n",
    "class GP():\n",
    "    def __init__(self, n_vx, **kwargs):\n",
    "        \"\"\"\n",
    "        Gaussian process        \n",
    "\n",
    "        N(m,K+nugget)\n",
    "        -> we construct m, the mean function\n",
    "        -> we construct K, the covariance function\n",
    "        -> with a nugget, for stability        \n",
    "\n",
    "        Args:\n",
    "            **kwargs: Optional parameters for controlling behavior, such as:\n",
    "                - psd_control: Method for ensuring positive semidefiniteness.\n",
    "                - gp_dtype: Data type for tensor conversion.\n",
    "                - kernel: Choice of covariance function (default: 'RBF').\n",
    "        \"\"\"\n",
    "        self.n_vx = tf.Variable(n_vx, dtype=tf.int32, name=\"n_vx\")\n",
    "\n",
    "        # Setup distance matrix and positive semidefinite control\n",
    "        self.psd_control   = kwargs.get('psd_control', 'euclidean')  # 'euclidean' or 'none'\n",
    "        self.eps           = kwargs.get('eps', 1e-6)\n",
    "        self.embedding_dim = kwargs.get('embedding_dim', 10)\n",
    "        self.gp_dtype   = kwargs.get('gp_dtype', tf.float64)\n",
    "\n",
    "        self.stat_kernel_list = []\n",
    "        self.lin_kernel_list = []\n",
    "        self.warp_kernel_list = []\n",
    "        self.mfunc_list = []\n",
    "        self.mfunc_bijector = tfb.Identity()\n",
    "        self.Xs = {}\n",
    "        self.dXs = {}\n",
    "        self.n_inducers = None  # if None, use all points\n",
    "        self.inducer_idx = None  # if None, use all points\n",
    "        self.nystrom = False\n",
    "        self.kernel_type = {}\n",
    "        self.pids = {}\n",
    "        # Index of parameters to be passed...\n",
    "        self.pids[0] = 'gpk_nugget' # Global nugget term\n",
    "        self.pids[1] = 'mfunc_mean' # Global mean term \n",
    "        self.pids_inv = {}\n",
    "        self._update_pids_inv()\n",
    "        self.return_log_prob = self._return_log_prob_unfixed # by default, return log prob unfixed...\n",
    "        self.gp_prior_dist = None\n",
    "\n",
    "    def _update_pids_inv(self):\n",
    "        self.pids_inv = {}\n",
    "        self.pids_inv = {v:k for k,v in self.pids.items()}\n",
    "    \n",
    "    def update_n_vx(self, new_value):\n",
    "        self.n_vx.assign(new_value)\n",
    "                        \n",
    "    # **************** MEAN FUNCTIONS ***************\n",
    "    def add_xid_linear_mfunc(self, xid, **kwargs):\n",
    "        ''' add linear mean function\n",
    "        '''\n",
    "        Xs = kwargs.get('Xs', None)\n",
    "        Xs = tf.convert_to_tensor(Xs, dtype=self.gp_dtype)\n",
    "        if len(Xs.shape) == 1:\n",
    "            Xs = tf.expand_dims(Xs, axis=-1)\n",
    "        self.Xs[xid] = Xs\n",
    "        self.mfunc_list.append(xid)        \n",
    "        Ds = self.Xs[xid].shape[1]\n",
    "        for i in range(Ds):\n",
    "            self.pids[len(self.pids)] = f'mfunc{xid}_slope{i}'\n",
    "\n",
    "        # Update the inverse dictionary\n",
    "        self._update_pids_inv()\n",
    "    \n",
    "    @tf.function\n",
    "    def _return_mfunc(self, **kwargs):\n",
    "        '''Return the mean function\n",
    "        '''\n",
    "        # Start of with zero then add global mean\n",
    "        m_out = tf.zeros(self.n_vx, dtype=self.gp_dtype) + tf.cast(kwargs['mfunc_mean'], self.gp_dtype) # global mean...\n",
    "        # then add any regressors...\n",
    "        for m in self.mfunc_list:\n",
    "            slopes = tf.stack([kwargs[f'mfunc{m}_slope{i}'] for i in range(self.Xs[m].shape[1])], axis=0)  # [D]\n",
    "            m_out += tf.reduce_sum(tf.cast(slopes, dtype=self.gp_dtype) * tf.transpose(self.Xs[m]), axis=0) \n",
    "        return self.mfunc_bijector(tf.cast(m_out, dtype=tf.float32))\n",
    "    \n",
    "    def add_mfunc_bijector(self, bijector_type, **kwargs):\n",
    "        ''' add transformations to parameters so that they are fit smoothly        \n",
    "        \n",
    "        identity        - do nothing\n",
    "        softplus        - don't let anything be negative\n",
    "\n",
    "        '''\n",
    "        if bijector_type == 'identity':\n",
    "            self.mfunc_bijector = tfb.Identity()        \n",
    "        elif bijector_type == 'softplus':\n",
    "            # Don't let anything be negative\n",
    "            self.mfunc_bijector = tfb.Softplus()\n",
    "        elif bijector_type == 'sigmoid':\n",
    "            self.mfunc_bijector = tfb.Sigmoid(\n",
    "                low=kwargs.get('low'), high=kwargs.get('high'),\n",
    "            )\n",
    "        else:\n",
    "            self.mfunc_bijector = bijector_type\n",
    "\n",
    "    # *************************************************\n",
    "    # *************************************************\n",
    "    # *************************************************\n",
    "    \n",
    "    # *** KERNELS ***\n",
    "    # -> add Stationary kernels \n",
    "    def add_xid_stationary_kernel(self, xid, **kwargs):\n",
    "        ''' add a kernel \n",
    "        '''\n",
    "        Xs = kwargs.get('Xs', None)\n",
    "        dXs = kwargs.get('dXs', None)\n",
    "        psd_control = kwargs.get('psd_control', self.psd_control)\n",
    "        embedding_dim = kwargs.get('embedding_dim', self.embedding_dim)\n",
    "        self.kernel_type[xid] = kwargs.get('kernel_type', 'RBF')                \n",
    "        self.stat_kernel_list.append(xid)\n",
    "\n",
    "        if dXs is None:\n",
    "            # Get distances from \n",
    "            dXs = compute_euclidean_distance_matrix(Xs[...,np.newaxis])        \n",
    "        if psd_control == 'euclidean':\n",
    "            print('Embedding in Euclidean space...')\n",
    "            dXs = mds_embedding(dXs, embedding_dim)\n",
    "            dXs = compute_euclidean_distance_matrix(dXs)\n",
    "        self.dXs[xid] = tf.convert_to_tensor(dXs, dtype=self.gp_dtype)\n",
    "        self.dXs[xid] = (self.dXs[xid] + tf.transpose(self.dXs[xid])) / 2.0        \n",
    "        # Add a lengthscale & a variance\n",
    "        self.pids[len(self.pids)] = f'gpk{xid}_l'\n",
    "        self.pids[len(self.pids)] = f'gpk{xid}_v'\n",
    "\n",
    "        # Update the inverse dictionary\n",
    "        self._update_pids_inv()        \n",
    "    \n",
    "    # # -> add Linear kernels\n",
    "    # def add_xid_linear_kernel(self, xid, **kwargs):\n",
    "    #     ''' add a kernel\n",
    "    #     '''\n",
    "    #     Xs = kwargs.get('Xs', None)\n",
    "    #     self.kernel_type[xid] = 'linear'\n",
    "    #     self.lin_kernel_list.append(xid)\n",
    "        \n",
    "    #     self.Xs[xid] = tf.expand_dims(tf.convert_to_tensor(Xs, dtype=self.gp_dtype), axis=1)\n",
    "    #     # Add a lengthscale & a variance\n",
    "    #     self.pids[len(self.pids)] = f'gpk{xid}_slope'\n",
    "    #     self.pids[len(self.pids)] = f'gpk{xid}_const'\n",
    "\n",
    "    #     # Update the inverse dictionary\n",
    "    #     self._update_pids_inv()    \n",
    "    \n",
    "    def add_xid_warp_kernel(self, xid, Xs, **kwargs):\n",
    "        self.Xs[xid] = tf.convert_to_tensor(Xs, dtype=self.gp_dtype)\n",
    "        self.pids[len(self.pids)] = f'gpk{xid}_v'\n",
    "        # Distance for RBF type kernel comes from warped LBO\n",
    "        for i in range(self.Xs[xid].shape[1]):\n",
    "            self.pids[len(self.pids)] = f'gpk{xid}_w{i}' \n",
    "        self.warp_kernel_list.append(xid)\n",
    "        self._update_pids_inv()\n",
    "                        \n",
    "\n",
    "    def add_nystrom_approximation(self, n_inducers, inducer_idx=None):\n",
    "        ''' Use nystrom approximation to speed up the GP\n",
    "        '''\n",
    "        self.n_inducers = n_inducers\n",
    "        self.inducer_idx = inducer_idx\n",
    "        self.nystrom = True\n",
    "        if self.inducer_idx is not None:\n",
    "            self.inducer_idx = tf.convert_to_tensor(self.inducer_idx, dtype=tf.int32)\n",
    "        else:\n",
    "            self.inducer_idx = tf.random.shuffle(tf.range(self.n_vx))[:self.n_inducers]\n",
    "        self.return_log_prob = self._return_log_prob_nystrom\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _return_sigma_full(self, **kwargs):\n",
    "        ''' Putting all the kernels together - > return the full covariance matrix\n",
    "        '''\n",
    "        # Start covariance matrix from zero...\n",
    "        s_out = tf.zeros((self.n_vx,self.n_vx), dtype=self.gp_dtype)\n",
    "        \n",
    "        # Add in any linear kernels\n",
    "        for s in self.lin_kernel_list:\n",
    "            s_out += self._return_sigma_xid_linear(\n",
    "                gpk_slope=kwargs[f'gpk{s}_slope'],\n",
    "                gpk_const=kwargs[f'gpk{s}_const'],\n",
    "                Xs=self.Xs[s],\n",
    "            )\n",
    "        \n",
    "        # # Add in any stationary kernels (e.g., RBF)\n",
    "        # for s in self.stat_kernel_list:\n",
    "        #     s_out += self._return_sigma_xid_stationary(\n",
    "        #         gpk_l=kwargs[f'gpk{s}_l'],\n",
    "        #         gpk_v=kwargs[f'gpk{s}_v'],\n",
    "        #         dXs=self.dXs[s],\n",
    "        #         kernel_type=self.kernel_type[s]\n",
    "        #     )\n",
    "        for s in self.warp_kernel_list:\n",
    "            s_kernel_type = s.split('_')[-1]\n",
    "            dXs = self._return_warp_dXs(\n",
    "                s, **kwargs,\n",
    "            )\n",
    "            s_out += self._return_sigma_xid_stationary(\n",
    "                gpk_l=1.0,\n",
    "                gpk_v=kwargs[f'gpk{s}_v'],\n",
    "                dXs=dXs,\n",
    "                kernel_type=s_kernel_type\n",
    "            )            \n",
    "        # Add the nugget term\n",
    "        s_out += tf.linalg.diag(tf.ones(self.n_vx, dtype=self.gp_dtype)) * tf.cast(self.eps + kwargs[f'gpk_nugget'], dtype=self.gp_dtype)\n",
    "        return s_out        \n",
    "    \n",
    "    @tf.function\n",
    "    def _return_warp_dXs(self, xid, **kwargs):\n",
    "        # [1] Weighted sum of eigenvectors (LBOwarp)\n",
    "        wX = tf.stack(\n",
    "            [kwargs[f\"gpk{xid}_w{i}\"] for i in range(self.Xs[xid].shape[1])],\n",
    "            axis=0\n",
    "        )\n",
    "        wX = tf.cast(wX, dtype=self.gp_dtype)\n",
    "        # Warped distances\n",
    "        warp_X = tf.matmul(self.Xs[xid], wX) # [N, 1]\n",
    "        warp_X = tf.squeeze(warp_X, axis=-1) # [N,]            \n",
    "        warp_dXs = compute_euclidean_distance_matrix(warp_X[...,tf.newaxis])                \n",
    "        return warp_dXs \n",
    "    \n",
    "    @tf.function\n",
    "    def _return_sigma_xid_stationary(self, gpk_l, gpk_v, dXs, kernel_type):\n",
    "        \"\"\"\n",
    "        Computes the covariance matrix using the chosen kernel.\n",
    "\n",
    "        Args:\n",
    "            gp_l (float): Lengthscale parameter.\n",
    "            gp_v (float): Variance parameter.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Covariance matrix.\n",
    "        \"\"\"\n",
    "        gpk_v = tf.cast(gpk_v, dtype=self.gp_dtype)\n",
    "        gpk_l = tf.cast(gpk_l, dtype=self.gp_dtype)\n",
    "\n",
    "        if kernel_type == 'RBF':\n",
    "            cov_matrix = tf.square(gpk_v) * tf.exp(\n",
    "                -tf.square(dXs) / (2.0 * tf.square(gpk_l))\n",
    "            )\n",
    "        elif kernel_type == 'matern52':\n",
    "            sqrt5 = tf.cast(tf.sqrt(5.0), dtype=self.gp_dtype)\n",
    "            frac1 = (sqrt5 * dXs) / gpk_l\n",
    "            frac2 = (5.0 * tf.square(dXs)) / (3.0 * tf.square(gpk_l))\n",
    "            cov_matrix = tf.square(gpk_v) * (1 + frac1 + frac2) * tf.exp(-frac1)\n",
    "        elif kernel_type == 'laplace':\n",
    "            cov_matrix = tf.square(gpk_v) * tf.exp(-dXs / gpk_l)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel: {}\".format(kernel_type))\n",
    "        # Add nugget term for numerical stability\n",
    "        return cov_matrix \n",
    "    \n",
    "    # @tf.function\n",
    "    # def _return_sigma_xid_linear(self, gpk_slope, gpk_const, Xs):\n",
    "    #     '''linear kernel\n",
    "    #     '''\n",
    "    #     gpk_slope = tf.cast(gpk_slope, dtype=self.gp_dtype)\n",
    "    #     gpk_const = tf.cast(gpk_const, dtype=self.gp_dtype)        \n",
    "    #     cov_matrix = gpk_slope**2 * (Xs-gpk_const) * (tf.transpose(Xs) - gpk_const)\n",
    "    #     return cov_matrix\n",
    "\n",
    "    def set_log_prob_fixed(self,**kwargs):\n",
    "        # Create a one off covariance matrix -> then use it to get probability each time...\n",
    "        # Get cov matrix\n",
    "        self.cov_matrix = self._return_sigma_full(**kwargs)\n",
    "        self.chol = tf.linalg.cholesky(tf.cast(self.cov_matrix, dtype=self.gp_dtype))\n",
    "        # Get mean vector\n",
    "        self.m_vect = self._return_mfunc(**kwargs)\n",
    "\n",
    "        self.gp_prior_dist = tfd.MultivariateNormalTriL(\n",
    "            loc=tf.squeeze(tf.cast(self.m_vect, dtype=tf.float32)), \n",
    "            scale_tril=tf.cast(self.chol, dtype=tf.float32),\n",
    "            allow_nan_stats=False,\n",
    "        )\n",
    "        self.return_log_prob = self._return_log_prob_fixed\n",
    "        \n",
    "    @tf.function\n",
    "    def _return_log_prob_nystrom(self, parameter, **kwargs):\n",
    "        ''' Return the log probability using nystrom approximation\n",
    "        '''\n",
    "        gpk_nugget = tf.cast(kwargs['gpk_nugget'], dtype=self.gp_dtype)        \n",
    "        m_vect = self._return_mfunc(**kwargs)        \n",
    "        parameter_dm = tf.cast(parameter - m_vect, self.gp_dtype) # remove mean function from parameter\n",
    "\n",
    "        K_full = self._return_sigma_full(**kwargs) # we do the nugget later, so have to remove it here...        \n",
    "        # might change this later...\n",
    "        K_full -= tf.linalg.diag(tf.ones(self.n_vx, dtype=self.gp_dtype)) * tf.cast(self.eps + kwargs[f'gpk_nugget'], dtype=self.gp_dtype)\n",
    "        A=tf.gather(tf.gather(K_full, self.inducer_idx, axis=0), self.inducer_idx, axis=1)\n",
    "        B=tf.gather(K_full, self.inducer_idx, axis=1)\n",
    "        # Add small jitter to A for PD-ness\n",
    "        A += tf.cast(self.eps, self.gp_dtype) * tf.eye(self.n_inducers, dtype=self.gp_dtype)\n",
    "\n",
    "        # Build S = A + (1/nugget) B^T B  (m x m)\n",
    "        BtB = tf.matmul(tf.transpose(B), B)   # (m,m)\n",
    "        S = A + (1.0 / gpk_nugget) * BtB\n",
    "\n",
    "        # Cholesky S\n",
    "        Ls = tf.linalg.cholesky(S) # (m,m)\n",
    "        # Solve S x = B^T y\n",
    "        Bt_y = tf.matmul(tf.transpose(B), tf.expand_dims(parameter_dm, -1))  # (m,1)\n",
    "        x = tf.linalg.cholesky_solve(Ls, Bt_y)                       # (m,1)\n",
    "\n",
    "        # Compute quadratic term via Woodbury K^{-1} y = (1/sigma2) y - (1/sigma2^2) B x\n",
    "        Bx = tf.matmul(B, x)                                        # (n,1)\n",
    "        v = (1.0 / gpk_nugget) * tf.expand_dims(parameter_dm, -1) - (1.0 / (gpk_nugget * gpk_nugget)) * Bx\n",
    "        quad = tf.squeeze(tf.matmul(tf.transpose(tf.expand_dims(parameter_dm, -1)), v))  # scalar\n",
    "        # Log-determinant via determinant lemma:\n",
    "        # log|K| = n log sigma2 - log|A| + log|S|\n",
    "        La = tf.linalg.cholesky(A)\n",
    "        logdetA = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(La)))\n",
    "        logdetS = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(Ls)))\n",
    "        n_float = tf.cast(self.n_vx, self.gp_dtype)\n",
    "        logdetK = n_float * tf.math.log(gpk_nugget) - logdetA + logdetS        \n",
    "        log2pi = tf.math.log(2.0 * tf.constant(math.pi, dtype=self.gp_dtype))\n",
    "        logp = -0.5 * quad - 0.5 * logdetK - 0.5 * n_float * log2pi\n",
    "        return tf.cast(tf.reshape(logp, []), tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def _return_log_prob_unfixed(self, parameter, **kwargs):\n",
    "        \"\"\"\n",
    "        Unfixed parameters using TensorFlow distribution.\n",
    "        Recompute covariance and Cholesky decomposition on the fly.\n",
    "        Optionally uses random selection of n_inducers for sparse GP approximation.\n",
    "        \"\"\"\n",
    "        # Get cov matrix\n",
    "        cov_matrix = self._return_sigma_full(**kwargs)\n",
    "        chol = tf.linalg.cholesky(tf.cast(cov_matrix, dtype=self.gp_dtype))\n",
    "        # Get mean vector\n",
    "        m_vect = self._return_mfunc(**kwargs)\n",
    "        gp_prior_dist = tfd.MultivariateNormalTriL(\n",
    "            loc=tf.squeeze(tf.cast(m_vect, dtype=tf.float32)), #tf.fill([self.n_vx], tf.squeeze(m_vect)),\n",
    "            scale_tril=tf.cast(chol, dtype=tf.float32),\n",
    "            allow_nan_stats=False,\n",
    "        )\n",
    "        return gp_prior_dist.log_prob(parameter)\n",
    "\n",
    "    @tf.function\n",
    "    def _return_log_prob_fixed(self, parameter, **kwargs):\n",
    "        \"\"\"\n",
    "        Unfixed parameters using TensorFlow distribution.\n",
    "        Recompute covariance and Cholesky decomposition on the fly.\n",
    "        Optionally uses random selection of n_inducers for sparse GP approximation.\n",
    "        \"\"\"\n",
    "        return self.gp_prior_dist.log_prob(parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea14418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Nystrom approximation...\n",
      "Nystrom approximation loop took: 0.2023 seconds\n",
      "\n",
      "Timing unfixed calculation...\n",
      "Unfixed calculation loop took: 0.5916 seconds\n",
      "\n",
      "--- Comparison ---\n",
      "The Nystrom approximation was faster by a factor of: 2.92\n",
      "\n",
      "Correlation between Nystrom and Unfixed results: corr=1.0000, mse=4.437234792931122e-07\n"
     ]
    }
   ],
   "source": [
    "n_loop = 10\n",
    "nvx = 1000\n",
    "m = 90\n",
    "\n",
    "x = np.random.rand(nvx)\n",
    "dXs = x[None,:] - x[...,None]\n",
    "\n",
    "gpN = GP(n_vx=nvx)\n",
    "gpN.add_xid_stationary_kernel(\n",
    "    xid='s', dXs=dXs, psd_control=None, \n",
    ")\n",
    "gpN.add_nystrom_approximation(n_inducers=m)\n",
    "\n",
    "# Generate all random parameters in advance\n",
    "random_params = [np.random.rand(nvx) for _ in range(n_loop)]\n",
    "\n",
    "# --- Timing the Nystrom approximation loop ---\n",
    "from time import time\n",
    "print(\"Timing Nystrom approximation...\")\n",
    "start_nystrom = time()\n",
    "No = []\n",
    "for i in range(n_loop):\n",
    "    eg_rand = random_params[i]\n",
    "    No.append(gpN._return_log_prob_nystrom(\n",
    "        parameter=tf.cast(eg_rand, tf.float32),\n",
    "        mfunc_mean=0.0,\n",
    "        gpks_l=1.0,\n",
    "        gpks_v=1.0,\n",
    "        gpk_nugget=0.1,\n",
    "    ).numpy())\n",
    "end_nystrom = time()\n",
    "nystrom_time = end_nystrom - start_nystrom\n",
    "print(f\"Nystrom approximation loop took: {nystrom_time:.4f} seconds\")\n",
    "\n",
    "# --- Timing the unfixed loop ---\n",
    "print(\"\\nTiming unfixed calculation...\")\n",
    "start_unfixed = time()\n",
    "Fo = []\n",
    "for i in range(n_loop):\n",
    "    # Use the same random parameter as the Nystrom loop\n",
    "    eg_rand = random_params[i]\n",
    "    Fo.append(gpN._return_log_prob_unfixed(\n",
    "        parameter=tf.cast(eg_rand, tf.float32),\n",
    "        mfunc_mean=0.0,\n",
    "        gpks_l=1.0,\n",
    "        gpks_v=1.0,\n",
    "        gpk_nugget=0.1,\n",
    "    ).numpy())\n",
    "end_unfixed = time()\n",
    "unfixed_time = end_unfixed - start_unfixed\n",
    "print(f\"Unfixed calculation loop took: {unfixed_time:.4f} seconds\")\n",
    "\n",
    "# --- Comparing the results ---\n",
    "print(\"\\n--- Comparison ---\")\n",
    "if nystrom_time < unfixed_time:\n",
    "    print(f\"The Nystrom approximation was faster by a factor of: {unfixed_time / nystrom_time:.2f}\")\n",
    "elif unfixed_time < nystrom_time:\n",
    "    print(f\"The unfixed calculation was faster by a factor of: {nystrom_time / unfixed_time:.2f}\")\n",
    "else:\n",
    "    print(\"The two calculations took approximately the same amount of time.\")\n",
    "No, Fo = np.array(No), np.array(Fo)\n",
    "# You can now calculate the correlation if you wish\n",
    "correlation = np.corrcoef(No, Fo)[0, 1]\n",
    "mse=(np.diff(No-Fo)**2).mean()\n",
    "print(f\"\\nCorrelation between Nystrom and Unfixed results: corr={correlation:.4f}, mse={mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON bcoder002b",
   "language": "python",
   "name": "bcoder002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
